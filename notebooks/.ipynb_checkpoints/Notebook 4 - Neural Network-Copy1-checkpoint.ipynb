{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da70ee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2112 entries, 0 to 2111\n",
      "Data columns (total 13 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   ID         2112 non-null   int64  \n",
      " 1   Substrate  2112 non-null   object \n",
      " 2   Ceramic    2112 non-null   object \n",
      " 3   Thickness  2112 non-null   float64\n",
      " 4   Lsub       2112 non-null   float64\n",
      " 5   asub       2112 non-null   float64\n",
      " 6   bsub       2112 non-null   float64\n",
      " 7   Lcer       2112 non-null   float64\n",
      " 8   acer       2112 non-null   float64\n",
      " 9   bcer       2112 non-null   float64\n",
      " 10  L          2112 non-null   float64\n",
      " 11  a          2112 non-null   float64\n",
      " 12  b          2112 non-null   float64\n",
      "dtypes: float64(10), int64(1), object(2)\n",
      "memory usage: 214.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import math\n",
    "df = pd.read_csv('../data/data2.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d5ac35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Predict L\n",
    "X, y = df[['Thickness','Lsub', 'asub','bsub', 'Lcer', 'acer', 'bcer']], df[['L', 'a', 'b']]\n",
    "\n",
    "#Perform Scaling if needed\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc = StandardScaler()\n",
    "#X_sc = sc.fit_transform(X)\n",
    "\n",
    "#Splitting the data into train and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "099b5777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1689, 7) (1689, 3)\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "X, y = pd.DataFrame(df[['Thickness', 'Lsub', 'asub','bsub', 'Lcer', 'acer', 'bcer']]).to_numpy(), pd.DataFrame(df[['L','a','b']]).to_numpy()\n",
    "# summarize shape\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbec8088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1.771\n",
      ">1.350\n",
      ">1.494\n",
      ">1.767\n",
      ">1.170\n",
      ">1.900\n",
      ">1.629\n",
      ">1.658\n",
      ">1.406\n",
      ">1.613\n",
      ">1.690\n",
      ">1.347\n",
      ">1.882\n",
      ">1.706\n",
      ">1.804\n",
      "MAE: 1.612 (0.208)\n"
     ]
    }
   ],
   "source": [
    "# mlp for multi-output regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    \n",
    "    X, y = pd.DataFrame(df[['Thickness', 'Lsub', 'asub','bsub', 'Lcer', 'acer', 'bcer']]).to_numpy(), pd.DataFrame(df[['L','a','b']]).to_numpy()\n",
    "    return X, y\n",
    "\n",
    "# get the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs))\n",
    "\tmodel.compile(loss='mae', optimizer='adam')\n",
    "\treturn model\n",
    "\n",
    "# evaluate a model using repeated k-fold cross-validation\n",
    "def evaluate_model(X, y):\n",
    "\tresults = list()\n",
    "\tn_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\t# enumerate folds\n",
    "\tfor train_ix, test_ix in cv.split(X):\n",
    "\t\t# prepare data\n",
    "\t\tX_train, X_test = X[train_ix], X[test_ix]\n",
    "\t\ty_train, y_test = y[train_ix], y[test_ix]\n",
    "\t\t# define model\n",
    "\t\tmodel = get_model(n_inputs, n_outputs)\n",
    "\t\t# fit model\n",
    "\t\tmodel.fit(X_train, y_train, verbose=0, epochs=100)\n",
    "\t\t# evaluate model on test set\n",
    "\t\tmae = model.evaluate(X_test, y_test, verbose=0)\n",
    "\t\t# store result\n",
    "\t\tprint('>%.3f' % mae)\n",
    "\t\tresults.append(mae)\n",
    "\treturn results\n",
    "\n",
    "# load dataset\n",
    "X, y = get_dataset()\n",
    "# evaluate model\n",
    "results = evaluate_model(X, y)\n",
    "# summarize performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(results), std(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "617efeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes):\n",
    "    layers = []\n",
    "    \n",
    "    nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
    "    nodes = first_layer_nodes\n",
    "    for i in range(1, n_layers+1):\n",
    "        layers.append(math.ceil(nodes))\n",
    "        nodes = nodes + nodes_increment\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b42eef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createmodel(n_layers, first_layer_nodes, last_layer_nodes, activation_func, loss_func):\n",
    "    model = Sequential()\n",
    "    n_nodes = FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes)\n",
    "    for i in range(1, n_layers):\n",
    "        if i==1:\n",
    "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=activation_func))\n",
    "        else:\n",
    "            model.add(Dense(n_nodes[i-1], activation=activation_func))\n",
    "            \n",
    "    #Finally, the output layer should have a single node in binary classification\n",
    "    model.add(Dense(1, activation=activation_func))\n",
    "    model.compile(optimizer='adam', loss=loss_func, metrics = [\"accuracy\"]) #note: metrics could also be 'mse'\n",
    "    \n",
    "    return model\n",
    "\n",
    "##Wrap model into scikit-learn\n",
    "model =  KerasRegressor(build_fn=createmodel, verbose = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "943873ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_funcs = ['linear'] \n",
    "loss_funcs = ['mean_squared_error']\n",
    "param_grid = dict(n_layers=[1,2], first_layer_nodes = [20, 10, 7, 5, 2], last_layer_nodes = [20, 15, 10, 5],  activation_func = activation_funcs, loss_func = loss_funcs, batch_size = [20, 50], epochs = [10, 20, 50])\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42510034",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d627c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate Deep ANN model \n",
    "def make_regression_ann(Optimizer_trial):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=15, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(3, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Optimizer_trial)\n",
    "    return model\n",
    " \n",
    "###########################################\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    " \n",
    "# Listing all the parameters to try\n",
    "Parameter_Trials={'batch_size':[10,20,30],\n",
    "                      'epochs':[10,20],\n",
    "                    'Optimizer_trial':['adam', 'rmsprop']\n",
    "                 }\n",
    " \n",
    "# Creating the regression ANN model\n",
    "RegModel=KerasRegressor(make_regression_ann, verbose=0)\n",
    " \n",
    "###########################################\n",
    "from sklearn.metrics import make_scorer\n",
    " \n",
    "# Defining a custom function to calculate accuracy\n",
    "def Accuracy_Score(orig,pred):\n",
    "    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))\n",
    "    print('#'*70,'Accuracy:', 100-MAPE)\n",
    "    return(100-MAPE)\n",
    " \n",
    "custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n",
    " \n",
    "#########################################\n",
    "# Creating the Grid search space\n",
    "# See different scoring methods by using sklearn.metrics.SCORERS.keys()\n",
    "grid_search=GridSearchCV(estimator=RegModel, \n",
    "                         param_grid=Parameter_Trials, \n",
    "                         scoring='r2', \n",
    "                         cv=5)\n",
    " \n",
    "#########################################\n",
    "# Measuring how much time it took to find the best params\n",
    "import time\n",
    "StartTime=time.time()\n",
    " \n",
    "# Running Grid Search for different paramenters\n",
    "grid_search.fit(X,y, verbose=1)\n",
    " \n",
    "EndTime=time.time()\n",
    "print(\"########## Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes')\n",
    " \n",
    "print('### Printing Best parameters ###')\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c19284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train ,batch_size = 10, epochs = 20, verbose=0)\n",
    " \n",
    "\n",
    "Test_Data=PredictorScalerFit.inverse_transform(X_test)\n",
    " \n",
    "TestingData=pd.DataFrame(data=Test_Data, columns=Predictors)\n",
    "TestingData['L']=y_test_orig\n",
    "TestingData['PredictedL']=Predictions\n",
    "TestingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ec514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the absolute percent error\n",
    "APE=100*(abs(TestingData['L']-TestingData['PredictedL'])/TestingData['L'])\n",
    "TestingData['APE']=APE\n",
    " \n",
    "print('The Accuracy of ANN model is:', 100-np.mean(APE))\n",
    "TestingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(TestingData['L'], TestingData['PredictedL'], edgecolors=(0, 0, 0))\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], \"k--\", lw=4)\n",
    "ax.set_xlabel(\"Measured\")\n",
    "ax.set_ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5dea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(TestingData['L'], TestingData['APE'], edgecolors=(0, 0, 0))\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], \"k--\", lw=4)\n",
    "ax.set_xlabel(\"Measured\")\n",
    "ax.set_ylabel(\"APE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ebe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and training sets.\n",
    "\n",
    "x_label = ['Lsub','asub', 'bsub', 'Lcer','acer','bcer', 'Thickness']\n",
    "y_label = ['L']\n",
    "np.random.seed(100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[x_label],df[y_label],test_size=0.2)\n",
    "# Print the dimensions\n",
    "print('Training set dimensions X, y: ' + str(X_train.shape) + ' ' +str(y_train.shape))\n",
    "print('Test set dimensions X, y: ' + str(X_test.shape) + ' '+ str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regression model in Keras\n",
    "def regression_model():\n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=15, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(3, kernel_initializer='normal'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Use KerasRegressor wrapper (from Keras to sklearn)\n",
    "# The packages we use are meant to be run with sklearn models\n",
    "estimator = KerasRegressor(build_fn=regression_model, validation_split = 0.2, batch_size=100, epochs=100, verbose=0)\n",
    "history = estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e51439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history loss\n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(estimator, random_state=1).fit(X_train,y_train)\n",
    "eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp for multi-output regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    x_label = ['Lsub','asub', 'bsub', 'Lcer','acer','bcer', 'Thickness']\n",
    "    y_label = ['L','a','b']\n",
    "    X, y = df[x_label], df[y_label]\n",
    "    return X, y\n",
    "# summarize shape\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and training sets.\n",
    "\n",
    "x_label = ['Lsub','asub', 'bsub', 'Lcer','acer','bcer', 'Thickness']\n",
    "y_label = ['L','a','b']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[x_label],df[y_label],test_size=0.2)\n",
    "# Print the dimensions\n",
    "print('Training set dimensions X, y: ' + str(X_train.shape) + ' ' +str(y_train.shape))\n",
    "print('Test set dimensions X, y: ' + str(X_test.shape) + ' '+ str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regression model in Keras\n",
    "def regression_model():\n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=5, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    adam = optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Use KerasRegressor wrapper (from Keras to sklearn)\n",
    "# The packages we use are meant to be run with sklearn models\n",
    "estimator = KerasRegressor(build_fn=regression_model, validation_split = 0.2, batch_size=100, epochs=100, verbose=0)\n",
    "history = estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f736ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(estimator, random_state=1).fit(X_train,y_train)\n",
    "eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b68e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ca2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# SHAP expects model functions to take a 2D numpy array as input, so we define a wrapper function around the original Keras predict function.\n",
    "def f_wrapper(X):\n",
    "    return estimator.predict(X).flatten()\n",
    "\n",
    "# Too many input data - use a random slice\n",
    "# rather than use the whole training set to estimate expected values, we summarize with\n",
    "# a set of weighted kmeans, each weighted by the number of points they represent.\n",
    "X_train_summary = shap.kmeans(X_train, 20)\n",
    "\n",
    "# Compute Shap values\n",
    "explainer = shap.KernelExplainer(f_wrapper,X_train_summary)\n",
    "\n",
    "# Make plot with combined shap values\n",
    "# The training set is too big so let's sample it. We get enough point to draw conclusions\n",
    "X_train_sample = X_train.sample(400)\n",
    "shap_values  = explainer.shap_values(X_train_sample)\n",
    "shap.summary_plot(shap_values, X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc804508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X, y = get_dataset()\n",
    "n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "# get model\n",
    "model = get_model(n_inputs, n_outputs)\n",
    "# fit the model on all data\n",
    "model.fit(X, y, verbose=0, epochs=100)\n",
    "# make a prediction for new data\n",
    "row = []\n",
    "newX = asarray([row])\n",
    "yhat = model.predict(newX)\n",
    "print('Predicted: %s' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf345cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "X, y = df[['Thickness','Lsub', 'asub','bsub', 'Lcer', 'acer', 'bcer']].to_numpy(), df[['L', 'a', 'b']].to_numpy()\n",
    "# summarize shape\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34acd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(units=15, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(3, kernel_initializer='normal'))\n",
    "model.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87694e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=15, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(3, kernel_initializer='normal'))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dead839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model using repeated k-fold cross-validation\n",
    "def evaluate_model(X, y):\n",
    "    results = []\n",
    "    n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        # prepare data\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        # define model\n",
    "        model = get_model(n_inputs, n_outputs)\n",
    "        # fit model\n",
    "        model.fit(X_train, y_train, verbose=10, epochs=15)\n",
    "        # evaluate model on test set\n",
    "        mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "        # store result\n",
    "        print('>%.3f' % mae)\n",
    "        results.append(mae)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe155bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "results = evaluate_model(X, y)\n",
    "# summarize performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(results), std(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction for new data\n",
    "row = [0.3,70,2.2,9,71, 0.2, 10]\n",
    "newX = np.array([row])\n",
    "yhat = model.predict(newX)\n",
    "print('Predicted: %s' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51e6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
