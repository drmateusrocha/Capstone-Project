{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da70ee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2112 entries, 0 to 2111\n",
      "Data columns (total 13 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   ID         2112 non-null   int64  \n",
      " 1   Substrate  2112 non-null   object \n",
      " 2   Ceramic    2112 non-null   object \n",
      " 3   Thickness  2112 non-null   float64\n",
      " 4   Lsub       2112 non-null   float64\n",
      " 5   asub       2112 non-null   float64\n",
      " 6   bsub       2112 non-null   float64\n",
      " 7   Lcer       2112 non-null   float64\n",
      " 8   acer       2112 non-null   float64\n",
      " 9   bcer       2112 non-null   float64\n",
      " 10  L          2112 non-null   float64\n",
      " 11  a          2112 non-null   float64\n",
      " 12  b          2112 non-null   float64\n",
      "dtypes: float64(10), int64(1), object(2)\n",
      "memory usage: 214.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import math\n",
    "df = pd.read_csv('../data/data2.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d5ac35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Predict L\n",
    "X, y = df[['Thickness','Lsub', 'asub','bsub', 'Lcer', 'acer', 'bcer']], df[['L', 'a', 'b']]\n",
    "\n",
    "#Perform Scaling if needed\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc = StandardScaler()\n",
    "#X_sc = sc.fit_transform(X)\n",
    "\n",
    "#Splitting the data into train and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "099b5777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1689, 7) (1689, 3)\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "X, y = pd.DataFrame(df[['Thickness', 'Lsub', 'asub','bsub', 'Lcer', 'acer', 'bcer']]).to_numpy(), pd.DataFrame(df[['L','a','b']]).to_numpy()\n",
    "# summarize shape\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbec8088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1.771\n",
      ">1.350\n",
      ">1.494\n",
      ">1.767\n",
      ">1.170\n",
      ">1.900\n",
      ">1.629\n",
      ">1.658\n",
      ">1.406\n",
      ">1.613\n",
      ">1.690\n",
      ">1.347\n",
      ">1.882\n",
      ">1.706\n",
      ">1.804\n",
      "MAE: 1.612 (0.208)\n"
     ]
    }
   ],
   "source": [
    "# mlp for multi-output regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    \n",
    "    X, y = pd.DataFrame(df[['Thickness', 'Lsub', 'asub','bsub', 'Lcer', 'acer', 'bcer']]).to_numpy(), pd.DataFrame(df[['L','a','b']]).to_numpy()\n",
    "    return X, y\n",
    "\n",
    "# get the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs))\n",
    "\tmodel.compile(loss='mae', optimizer='adam')\n",
    "\treturn model\n",
    "\n",
    "# evaluate a model using repeated k-fold cross-validation\n",
    "def evaluate_model(X, y):\n",
    "\tresults = list()\n",
    "\tn_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\t# enumerate folds\n",
    "\tfor train_ix, test_ix in cv.split(X):\n",
    "\t\t# prepare data\n",
    "\t\tX_train, X_test = X[train_ix], X[test_ix]\n",
    "\t\ty_train, y_test = y[train_ix], y[test_ix]\n",
    "\t\t# define model\n",
    "\t\tmodel = get_model(n_inputs, n_outputs)\n",
    "\t\t# fit model\n",
    "\t\tmodel.fit(X_train, y_train, verbose=0, epochs=100)\n",
    "\t\t# evaluate model on test set\n",
    "\t\tmae = model.evaluate(X_test, y_test, verbose=0)\n",
    "\t\t# store result\n",
    "\t\tprint('>%.3f' % mae)\n",
    "\t\tresults.append(mae)\n",
    "\treturn results\n",
    "\n",
    "# load dataset\n",
    "X, y = get_dataset()\n",
    "# evaluate model\n",
    "results = evaluate_model(X, y)\n",
    "# summarize performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(results), std(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "617efeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes):\n",
    "    layers = []\n",
    "    \n",
    "    nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
    "    nodes = first_layer_nodes\n",
    "    for i in range(1, n_layers+1):\n",
    "        layers.append(math.ceil(nodes))\n",
    "        nodes = nodes + nodes_increment\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b42eef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createmodel(n_layers, first_layer_nodes, last_layer_nodes, activation_func, loss_func):\n",
    "    model = Sequential()\n",
    "    n_nodes = FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes)\n",
    "    for i in range(1, n_layers):\n",
    "        if i==1:\n",
    "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=activation_func))\n",
    "        else:\n",
    "            model.add(Dense(n_nodes[i-1], activation=activation_func))\n",
    "            \n",
    "    #Finally, the output layer should have a single node in binary classification\n",
    "    model.add(Dense(3, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss=loss_func, metrics = [\"accuracy\"]) #note: metrics could also be 'mse'\n",
    "    \n",
    "    return model\n",
    "\n",
    "##Wrap model into scikit-learn\n",
    "model =  KerasRegressor(build_fn=createmodel, verbose = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "943873ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_funcs = ['relu']#['sigmoid', 'relu', 'tanh'] \n",
    "loss_funcs = ['mean_squared_error'] #['mean_squared_error','hinge','mae']\n",
    "param_grid = dict(n_layers=[1,2], first_layer_nodes = [7, 5], last_layer_nodes = [5, 3],  activation_func = activation_funcs, loss_func = loss_funcs, batch_size = [50], epochs = [10])\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid, cv=10, n_jobs=-1, verbose=1, scoring = \"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42510034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exception calling callback for <Future at 0x1e7e09d9220 state=finished raised TerminatedWorkerError>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 625, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py\", line 359, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py\", line 794, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 531, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\reusable_executor.py\", line 177, in submit\n",
      "    return super(_ReusablePoolExecutor, self).submit(\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 1115, in submit\n",
      "    raise self._flags.broken\n",
      "joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n",
      "\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\concurrent\\futures\\_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\_base.py:625\u001b[0m, in \u001b[0;36mFuture._invoke_callbacks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done_callbacks:\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 625\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m         LOGGER\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexception calling callback for \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py:359\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.__call__\u001b[1;34m(self, out)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 359\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py:794\u001b[0m, in \u001b[0;36mParallel.dispatch_next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch_next\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;124;03m\"\"\"Dispatch more data for parallel processing\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \n\u001b[0;32m    789\u001b[0m \u001b[38;5;124;03m    This method is meant to be called concurrently by the multiprocessing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    792\u001b[0m \n\u001b[0;32m    793\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 794\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_original_iterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\_parallel_backends.py:531\u001b[0m, in \u001b[0;36mLokyBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 531\u001b[0m     future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSafeFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m     future\u001b[38;5;241m.\u001b[39mget \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_future_result, future)\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\reusable_executor.py:177\u001b[0m, in \u001b[0;36m_ReusablePoolExecutor.submit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubmit\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit_resize_lock:\n\u001b[1;32m--> 177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(_ReusablePoolExecutor, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m    178\u001b[0m             fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:1115\u001b[0m, in \u001b[0;36mProcessPoolExecutor.submit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flags\u001b[38;5;241m.\u001b[39mshutdown_lock:\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flags\u001b[38;5;241m.\u001b[39mbroken \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1115\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flags\u001b[38;5;241m.\u001b[39mbroken\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flags\u001b[38;5;241m.\u001b[39mshutdown:\n\u001b[0;32m   1117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ShutdownExecutorError(\n\u001b[0;32m   1118\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot schedule new futures after shutdown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d627c66",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_score_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_score_\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(grid\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_score_'"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39ae18a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m score_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv_results_\u001b[49m)\n\u001b[0;32m      2\u001b[0m score_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank_test_score\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "score_df = pd.DataFrame(grid.cv_results_)\n",
    "score_df.sort_values(by='rank_test_score').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6c9e299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-24.490260</td>\n",
       "      <td>14.922815</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-25.036329</td>\n",
       "      <td>11.882321</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-23.875925</td>\n",
       "      <td>11.804245</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-24.278635</td>\n",
       "      <td>9.413140</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-51.353558</td>\n",
       "      <td>10.550953</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-74.233941</td>\n",
       "      <td>74.533497</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-161.150762</td>\n",
       "      <td>129.050585</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-60.770346</td>\n",
       "      <td>55.504804</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-206.444029</td>\n",
       "      <td>157.495241</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-165.809924</td>\n",
       "      <td>179.319228</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-117.522153</td>\n",
       "      <td>94.180486</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-725.065265</td>\n",
       "      <td>826.535223</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-241.122491</td>\n",
       "      <td>193.138596</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-79.065467</td>\n",
       "      <td>31.857363</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-94.885411</td>\n",
       "      <td>73.731444</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-110.547735</td>\n",
       "      <td>103.795524</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-1660.015878</td>\n",
       "      <td>927.089387</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-727.792342</td>\n",
       "      <td>694.000003</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-1102.576804</td>\n",
       "      <td>872.194213</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-1484.946265</td>\n",
       "      <td>176.345959</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score  std_test_score  \\\n",
       "0               NaN             NaN   \n",
       "1        -24.490260       14.922815   \n",
       "2               NaN             NaN   \n",
       "3        -25.036329       11.882321   \n",
       "4               NaN             NaN   \n",
       "5        -23.875925       11.804245   \n",
       "6               NaN             NaN   \n",
       "7        -24.278635        9.413140   \n",
       "8               NaN             NaN   \n",
       "9        -51.353558       10.550953   \n",
       "10              NaN             NaN   \n",
       "11       -74.233941       74.533497   \n",
       "12              NaN             NaN   \n",
       "13      -161.150762      129.050585   \n",
       "14              NaN             NaN   \n",
       "15       -60.770346       55.504804   \n",
       "16              NaN             NaN   \n",
       "17      -206.444029      157.495241   \n",
       "18              NaN             NaN   \n",
       "19      -165.809924      179.319228   \n",
       "20              NaN             NaN   \n",
       "21      -117.522153       94.180486   \n",
       "22              NaN             NaN   \n",
       "23      -725.065265      826.535223   \n",
       "24              NaN             NaN   \n",
       "25      -241.122491      193.138596   \n",
       "26              NaN             NaN   \n",
       "27       -79.065467       31.857363   \n",
       "28              NaN             NaN   \n",
       "29       -94.885411       73.731444   \n",
       "30              NaN             NaN   \n",
       "31      -110.547735      103.795524   \n",
       "32              NaN             NaN   \n",
       "33     -1660.015878      927.089387   \n",
       "34              NaN             NaN   \n",
       "35      -727.792342      694.000003   \n",
       "36              NaN             NaN   \n",
       "37     -1102.576804      872.194213   \n",
       "38              NaN             NaN   \n",
       "39     -1484.946265      176.345959   \n",
       "\n",
       "                                               params  \n",
       "0   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "1   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "2   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "3   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "4   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "5   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "6   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "7   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "8   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "9   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "10  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "11  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "12  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "13  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "14  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "15  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "16  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "17  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "18  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "19  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "20  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "21  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "22  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "23  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "24  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "25  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "26  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "27  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "28  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "29  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "30  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "31  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "32  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "33  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "34  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "35  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "36  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "37  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "38  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "39  {'activation_func': 'relu', 'batch_size': 50, ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate Deep ANN model \n",
    "def make_regression_ann(Optimizer_trial):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=15, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(3, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Optimizer_trial)\n",
    "    return model\n",
    " \n",
    "###########################################\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    " \n",
    "# Listing all the parameters to try\n",
    "Parameter_Trials={'batch_size':[10,20,30],\n",
    "                      'epochs':[10,20],\n",
    "                    'Optimizer_trial':['adam', 'rmsprop']\n",
    "                 }\n",
    " \n",
    "# Creating the regression ANN model\n",
    "RegModel=KerasRegressor(make_regression_ann, verbose=0)\n",
    " \n",
    "###########################################\n",
    "from sklearn.metrics import make_scorer\n",
    " \n",
    "# Defining a custom function to calculate accuracy\n",
    "def Accuracy_Score(orig,pred):\n",
    "    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))\n",
    "    print('#'*70,'Accuracy:', 100-MAPE)\n",
    "    return(100-MAPE)\n",
    " \n",
    "custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n",
    " \n",
    "#########################################\n",
    "# Creating the Grid search space\n",
    "# See different scoring methods by using sklearn.metrics.SCORERS.keys()\n",
    "grid_search=GridSearchCV(estimator=RegModel, \n",
    "                         param_grid=Parameter_Trials, \n",
    "                         scoring='r2', \n",
    "                         cv=5)\n",
    " \n",
    "#########################################\n",
    "# Measuring how much time it took to find the best params\n",
    "import time\n",
    "StartTime=time.time()\n",
    " \n",
    "# Running Grid Search for different paramenters\n",
    "grid_search.fit(X,y, verbose=1)\n",
    " \n",
    "EndTime=time.time()\n",
    "print(\"########## Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes')\n",
    " \n",
    "print('### Printing Best parameters ###')\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c19284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train ,batch_size = 10, epochs = 20, verbose=0)\n",
    " \n",
    "\n",
    "Test_Data=PredictorScalerFit.inverse_transform(X_test)\n",
    " \n",
    "TestingData=pd.DataFrame(data=Test_Data, columns=Predictors)\n",
    "TestingData['L']=y_test_orig\n",
    "TestingData['PredictedL']=Predictions\n",
    "TestingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ec514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the absolute percent error\n",
    "APE=100*(abs(TestingData['L']-TestingData['PredictedL'])/TestingData['L'])\n",
    "TestingData['APE']=APE\n",
    " \n",
    "print('The Accuracy of ANN model is:', 100-np.mean(APE))\n",
    "TestingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(TestingData['L'], TestingData['PredictedL'], edgecolors=(0, 0, 0))\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], \"k--\", lw=4)\n",
    "ax.set_xlabel(\"Measured\")\n",
    "ax.set_ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5dea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(TestingData['L'], TestingData['APE'], edgecolors=(0, 0, 0))\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], \"k--\", lw=4)\n",
    "ax.set_xlabel(\"Measured\")\n",
    "ax.set_ylabel(\"APE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ebe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and training sets.\n",
    "\n",
    "x_label = ['Lsub','asub', 'bsub', 'Lcer','acer','bcer', 'Thickness']\n",
    "y_label = ['L']\n",
    "np.random.seed(100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[x_label],df[y_label],test_size=0.2)\n",
    "# Print the dimensions\n",
    "print('Training set dimensions X, y: ' + str(X_train.shape) + ' ' +str(y_train.shape))\n",
    "print('Test set dimensions X, y: ' + str(X_test.shape) + ' '+ str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regression model in Keras\n",
    "def regression_model():\n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=15, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(3, kernel_initializer='normal'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Use KerasRegressor wrapper (from Keras to sklearn)\n",
    "# The packages we use are meant to be run with sklearn models\n",
    "estimator = KerasRegressor(build_fn=regression_model, validation_split = 0.2, batch_size=100, epochs=100, verbose=0)\n",
    "history = estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e51439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history loss\n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(estimator, random_state=1).fit(X_train,y_train)\n",
    "eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp for multi-output regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    x_label = ['Lsub','asub', 'bsub', 'Lcer','acer','bcer', 'Thickness']\n",
    "    y_label = ['L','a','b']\n",
    "    X, y = df[x_label], df[y_label]\n",
    "    return X, y\n",
    "# summarize shape\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and training sets.\n",
    "\n",
    "x_label = ['Lsub','asub', 'bsub', 'Lcer','acer','bcer', 'Thickness']\n",
    "y_label = ['L','a','b']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[x_label],df[y_label],test_size=0.2)\n",
    "# Print the dimensions\n",
    "print('Training set dimensions X, y: ' + str(X_train.shape) + ' ' +str(y_train.shape))\n",
    "print('Test set dimensions X, y: ' + str(X_test.shape) + ' '+ str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regression model in Keras\n",
    "def regression_model():\n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=5, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    adam = optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Use KerasRegressor wrapper (from Keras to sklearn)\n",
    "# The packages we use are meant to be run with sklearn models\n",
    "estimator = KerasRegressor(build_fn=regression_model, validation_split = 0.2, batch_size=100, epochs=100, verbose=0)\n",
    "history = estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f736ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(estimator, random_state=1).fit(X_train,y_train)\n",
    "eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b68e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ca2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# SHAP expects model functions to take a 2D numpy array as input, so we define a wrapper function around the original Keras predict function.\n",
    "def f_wrapper(X):\n",
    "    return estimator.predict(X).flatten()\n",
    "\n",
    "# Too many input data - use a random slice\n",
    "# rather than use the whole training set to estimate expected values, we summarize with\n",
    "# a set of weighted kmeans, each weighted by the number of points they represent.\n",
    "X_train_summary = shap.kmeans(X_train, 20)\n",
    "\n",
    "# Compute Shap values\n",
    "explainer = shap.KernelExplainer(f_wrapper,X_train_summary)\n",
    "\n",
    "# Make plot with combined shap values\n",
    "# The training set is too big so let's sample it. We get enough point to draw conclusions\n",
    "X_train_sample = X_train.sample(400)\n",
    "shap_values  = explainer.shap_values(X_train_sample)\n",
    "shap.summary_plot(shap_values, X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc804508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X, y = get_dataset()\n",
    "n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "# get model\n",
    "model = get_model(n_inputs, n_outputs)\n",
    "# fit the model on all data\n",
    "model.fit(X, y, verbose=0, epochs=100)\n",
    "# make a prediction for new data\n",
    "row = []\n",
    "newX = asarray([row])\n",
    "yhat = model.predict(newX)\n",
    "print('Predicted: %s' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf345cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "X, y = df[['Thickness','Lsub', 'asub','bsub', 'Lcer', 'acer', 'bcer']].to_numpy(), df[['L', 'a', 'b']].to_numpy()\n",
    "# summarize shape\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34acd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(units=15, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(3, kernel_initializer='normal'))\n",
    "model.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87694e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=15, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(3, kernel_initializer='normal'))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dead839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model using repeated k-fold cross-validation\n",
    "def evaluate_model(X, y):\n",
    "    results = []\n",
    "    n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        # prepare data\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        # define model\n",
    "        model = get_model(n_inputs, n_outputs)\n",
    "        # fit model\n",
    "        model.fit(X_train, y_train, verbose=10, epochs=15)\n",
    "        # evaluate model on test set\n",
    "        mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "        # store result\n",
    "        print('>%.3f' % mae)\n",
    "        results.append(mae)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe155bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "results = evaluate_model(X, y)\n",
    "# summarize performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(results), std(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction for new data\n",
    "row = [0.3,70,2.2,9,71, 0.2, 10]\n",
    "newX = np.array([row])\n",
    "yhat = model.predict(newX)\n",
    "print('Predicted: %s' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51e6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
