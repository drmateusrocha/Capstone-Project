{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "627d300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 23:28:01.128087: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2112 entries, 0 to 2111\n",
      "Data columns (total 13 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   ID         2112 non-null   int64  \n",
      " 1   Substrate  2112 non-null   object \n",
      " 2   Ceramic    2112 non-null   object \n",
      " 3   Thickness  2112 non-null   float64\n",
      " 4   Lsub       2112 non-null   float64\n",
      " 5   asub       2112 non-null   float64\n",
      " 6   bsub       2112 non-null   float64\n",
      " 7   Lcer       2112 non-null   float64\n",
      " 8   acer       2112 non-null   float64\n",
      " 9   bcer       2112 non-null   float64\n",
      " 10  L          2112 non-null   float64\n",
      " 11  a          2112 non-null   float64\n",
      " 12  b          2112 non-null   float64\n",
      "dtypes: float64(10), int64(1), object(2)\n",
      "memory usage: 214.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/data2.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa9f09a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 23:38:10.982496: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 0s 679us/step - loss: 40.9777 - accuracy: 0.0000e+00\n",
      "Epoch 2/150\n",
      "212/212 [==============================] - 0s 642us/step - loss: 14.4486 - accuracy: 0.7325\n",
      "Epoch 3/150\n",
      "212/212 [==============================] - 0s 637us/step - loss: 4.4810 - accuracy: 1.0000\n",
      "Epoch 4/150\n",
      "212/212 [==============================] - 0s 631us/step - loss: 3.4581 - accuracy: 1.0000\n",
      "Epoch 5/150\n",
      "212/212 [==============================] - 0s 641us/step - loss: 2.8361 - accuracy: 1.0000\n",
      "Epoch 6/150\n",
      "212/212 [==============================] - 0s 638us/step - loss: 2.4670 - accuracy: 1.0000\n",
      "Epoch 7/150\n",
      "212/212 [==============================] - 0s 643us/step - loss: 2.2698 - accuracy: 1.0000\n",
      "Epoch 8/150\n",
      "212/212 [==============================] - 0s 699us/step - loss: 2.1206 - accuracy: 1.0000\n",
      "Epoch 9/150\n",
      "212/212 [==============================] - 0s 682us/step - loss: 2.0253 - accuracy: 1.0000\n",
      "Epoch 10/150\n",
      "212/212 [==============================] - 0s 727us/step - loss: 1.9594 - accuracy: 1.0000\n",
      "Epoch 11/150\n",
      "212/212 [==============================] - 0s 694us/step - loss: 1.9295 - accuracy: 1.0000\n",
      "Epoch 12/150\n",
      "212/212 [==============================] - 0s 668us/step - loss: 1.8965 - accuracy: 1.0000\n",
      "Epoch 13/150\n",
      "212/212 [==============================] - 0s 723us/step - loss: 1.8883 - accuracy: 1.0000\n",
      "Epoch 14/150\n",
      "212/212 [==============================] - 0s 646us/step - loss: 1.8797 - accuracy: 1.0000\n",
      "Epoch 15/150\n",
      "212/212 [==============================] - 0s 672us/step - loss: 1.8761 - accuracy: 1.0000\n",
      "Epoch 16/150\n",
      "212/212 [==============================] - 0s 680us/step - loss: 1.8731 - accuracy: 1.0000\n",
      "Epoch 17/150\n",
      "212/212 [==============================] - 0s 662us/step - loss: 1.8656 - accuracy: 1.0000\n",
      "Epoch 18/150\n",
      "212/212 [==============================] - 0s 635us/step - loss: 1.8713 - accuracy: 1.0000\n",
      "Epoch 19/150\n",
      "212/212 [==============================] - 0s 657us/step - loss: 1.8656 - accuracy: 1.0000\n",
      "Epoch 20/150\n",
      "212/212 [==============================] - 0s 680us/step - loss: 1.8580 - accuracy: 1.0000\n",
      "Epoch 21/150\n",
      "212/212 [==============================] - 0s 646us/step - loss: 1.8630 - accuracy: 1.0000\n",
      "Epoch 22/150\n",
      "212/212 [==============================] - 0s 644us/step - loss: 1.8639 - accuracy: 1.0000\n",
      "Epoch 23/150\n",
      "212/212 [==============================] - 0s 697us/step - loss: 1.8655 - accuracy: 1.0000\n",
      "Epoch 24/150\n",
      "212/212 [==============================] - 0s 641us/step - loss: 1.8609 - accuracy: 1.0000\n",
      "Epoch 25/150\n",
      "212/212 [==============================] - 0s 707us/step - loss: 1.8614 - accuracy: 1.0000\n",
      "Epoch 26/150\n",
      "212/212 [==============================] - 0s 780us/step - loss: 1.8619 - accuracy: 1.0000\n",
      "Epoch 27/150\n",
      "212/212 [==============================] - 0s 978us/step - loss: 1.8661 - accuracy: 1.0000\n",
      "Epoch 28/150\n",
      "212/212 [==============================] - 0s 779us/step - loss: 1.8651 - accuracy: 1.0000\n",
      "Epoch 29/150\n",
      "212/212 [==============================] - 0s 780us/step - loss: 1.8558 - accuracy: 1.0000\n",
      "Epoch 30/150\n",
      "212/212 [==============================] - 0s 830us/step - loss: 1.8605 - accuracy: 1.0000\n",
      "Epoch 31/150\n",
      "212/212 [==============================] - 0s 906us/step - loss: 1.8610 - accuracy: 1.0000\n",
      "Epoch 32/150\n",
      "212/212 [==============================] - 0s 708us/step - loss: 1.8551 - accuracy: 1.0000\n",
      "Epoch 33/150\n",
      "212/212 [==============================] - 0s 731us/step - loss: 1.8549 - accuracy: 1.0000\n",
      "Epoch 34/150\n",
      "212/212 [==============================] - 0s 864us/step - loss: 1.8602 - accuracy: 1.0000\n",
      "Epoch 35/150\n",
      "212/212 [==============================] - 0s 769us/step - loss: 1.8569 - accuracy: 1.0000\n",
      "Epoch 36/150\n",
      "212/212 [==============================] - 0s 871us/step - loss: 1.8556 - accuracy: 1.0000\n",
      "Epoch 37/150\n",
      "212/212 [==============================] - 0s 875us/step - loss: 1.8563 - accuracy: 1.0000\n",
      "Epoch 38/150\n",
      "212/212 [==============================] - 0s 722us/step - loss: 1.8573 - accuracy: 1.0000\n",
      "Epoch 39/150\n",
      "212/212 [==============================] - 0s 756us/step - loss: 1.8505 - accuracy: 1.0000\n",
      "Epoch 40/150\n",
      "212/212 [==============================] - 0s 699us/step - loss: 1.8597 - accuracy: 1.0000\n",
      "Epoch 41/150\n",
      "212/212 [==============================] - 0s 693us/step - loss: 1.8508 - accuracy: 1.0000\n",
      "Epoch 42/150\n",
      "212/212 [==============================] - 0s 705us/step - loss: 1.8524 - accuracy: 1.0000\n",
      "Epoch 43/150\n",
      "212/212 [==============================] - 0s 697us/step - loss: 1.8521 - accuracy: 1.0000\n",
      "Epoch 44/150\n",
      "212/212 [==============================] - 0s 772us/step - loss: 1.8556 - accuracy: 1.0000\n",
      "Epoch 45/150\n",
      "212/212 [==============================] - 0s 917us/step - loss: 1.8553 - accuracy: 1.0000\n",
      "Epoch 46/150\n",
      "212/212 [==============================] - 0s 795us/step - loss: 1.8512 - accuracy: 1.0000\n",
      "Epoch 47/150\n",
      "212/212 [==============================] - 0s 871us/step - loss: 1.8531 - accuracy: 1.0000\n",
      "Epoch 48/150\n",
      "212/212 [==============================] - 0s 800us/step - loss: 1.8525 - accuracy: 1.0000\n",
      "Epoch 49/150\n",
      "212/212 [==============================] - 0s 951us/step - loss: 1.8567 - accuracy: 1.0000\n",
      "Epoch 50/150\n",
      "212/212 [==============================] - 0s 1ms/step - loss: 1.8505 - accuracy: 1.0000\n",
      "Epoch 51/150\n",
      "212/212 [==============================] - 0s 873us/step - loss: 1.8477 - accuracy: 1.0000\n",
      "Epoch 52/150\n",
      "212/212 [==============================] - 0s 688us/step - loss: 1.8504 - accuracy: 1.0000\n",
      "Epoch 53/150\n",
      "212/212 [==============================] - 0s 691us/step - loss: 1.8526 - accuracy: 1.0000\n",
      "Epoch 54/150\n",
      "212/212 [==============================] - 0s 648us/step - loss: 1.8517 - accuracy: 1.0000\n",
      "Epoch 55/150\n",
      "212/212 [==============================] - 0s 682us/step - loss: 1.8534 - accuracy: 1.0000\n",
      "Epoch 56/150\n",
      "212/212 [==============================] - 0s 684us/step - loss: 1.8548 - accuracy: 1.0000\n",
      "Epoch 57/150\n",
      "212/212 [==============================] - 0s 751us/step - loss: 1.8571 - accuracy: 1.0000\n",
      "Epoch 58/150\n",
      "212/212 [==============================] - 0s 672us/step - loss: 1.8539 - accuracy: 1.0000\n",
      "Epoch 59/150\n",
      "212/212 [==============================] - 0s 648us/step - loss: 1.8427 - accuracy: 1.0000\n",
      "Epoch 60/150\n",
      "212/212 [==============================] - 0s 677us/step - loss: 1.8545 - accuracy: 1.0000\n",
      "Epoch 61/150\n",
      "212/212 [==============================] - 0s 648us/step - loss: 1.8533 - accuracy: 1.0000\n",
      "Epoch 62/150\n",
      "212/212 [==============================] - 0s 644us/step - loss: 1.8588 - accuracy: 1.0000\n",
      "Epoch 63/150\n",
      "212/212 [==============================] - 0s 656us/step - loss: 1.8429 - accuracy: 1.0000\n",
      "Epoch 64/150\n",
      "212/212 [==============================] - 0s 649us/step - loss: 1.8555 - accuracy: 1.0000\n",
      "Epoch 65/150\n",
      "212/212 [==============================] - 0s 640us/step - loss: 1.8527 - accuracy: 1.0000\n",
      "Epoch 66/150\n",
      "212/212 [==============================] - 0s 697us/step - loss: 1.8511 - accuracy: 1.0000\n",
      "Epoch 67/150\n",
      "212/212 [==============================] - 0s 677us/step - loss: 1.8530 - accuracy: 1.0000\n",
      "Epoch 68/150\n",
      "212/212 [==============================] - 0s 650us/step - loss: 1.8485 - accuracy: 1.0000\n",
      "Epoch 69/150\n",
      "212/212 [==============================] - 0s 650us/step - loss: 1.8465 - accuracy: 1.0000\n",
      "Epoch 70/150\n",
      "212/212 [==============================] - 0s 660us/step - loss: 1.8505 - accuracy: 1.0000\n",
      "Epoch 71/150\n",
      "212/212 [==============================] - 0s 636us/step - loss: 1.8505 - accuracy: 1.0000\n",
      "Epoch 72/150\n",
      "212/212 [==============================] - 0s 676us/step - loss: 1.8492 - accuracy: 1.0000\n",
      "Epoch 73/150\n",
      "212/212 [==============================] - 0s 691us/step - loss: 1.8450 - accuracy: 1.0000\n",
      "Epoch 74/150\n",
      "212/212 [==============================] - 0s 683us/step - loss: 1.8543 - accuracy: 1.0000\n",
      "Epoch 75/150\n",
      "212/212 [==============================] - 0s 664us/step - loss: 1.8508 - accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "212/212 [==============================] - 0s 647us/step - loss: 1.8562 - accuracy: 1.0000\n",
      "Epoch 77/150\n",
      "212/212 [==============================] - 0s 668us/step - loss: 1.8596 - accuracy: 1.0000\n",
      "Epoch 78/150\n",
      "212/212 [==============================] - 0s 643us/step - loss: 1.8486 - accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "212/212 [==============================] - 0s 632us/step - loss: 1.8654 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/150\n",
      "212/212 [==============================] - 0s 625us/step - loss: 1.8468 - accuracy: 1.0000\n",
      "Epoch 81/150\n",
      "212/212 [==============================] - 0s 626us/step - loss: 1.8511 - accuracy: 1.0000\n",
      "Epoch 82/150\n",
      "212/212 [==============================] - 0s 625us/step - loss: 1.8486 - accuracy: 1.0000\n",
      "Epoch 83/150\n",
      "212/212 [==============================] - 0s 648us/step - loss: 1.8544 - accuracy: 1.0000\n",
      "Epoch 84/150\n",
      "212/212 [==============================] - 0s 658us/step - loss: 1.8453 - accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "212/212 [==============================] - 0s 638us/step - loss: 1.8503 - accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "212/212 [==============================] - 0s 651us/step - loss: 1.8417 - accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "212/212 [==============================] - 0s 685us/step - loss: 1.8496 - accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "212/212 [==============================] - 0s 700us/step - loss: 1.8467 - accuracy: 1.0000\n",
      "Epoch 89/150\n",
      "212/212 [==============================] - 0s 696us/step - loss: 1.8472 - accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "212/212 [==============================] - 0s 689us/step - loss: 1.8530 - accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "212/212 [==============================] - 0s 723us/step - loss: 1.8527 - accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "212/212 [==============================] - 0s 635us/step - loss: 1.8642 - accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "212/212 [==============================] - 0s 624us/step - loss: 1.8526 - accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "212/212 [==============================] - 0s 630us/step - loss: 1.8451 - accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "212/212 [==============================] - 0s 675us/step - loss: 1.8430 - accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "212/212 [==============================] - 0s 663us/step - loss: 1.8495 - accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "212/212 [==============================] - 0s 664us/step - loss: 1.8525 - accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "212/212 [==============================] - 0s 668us/step - loss: 1.8475 - accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "212/212 [==============================] - 0s 735us/step - loss: 1.8425 - accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "212/212 [==============================] - 0s 701us/step - loss: 1.8487 - accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "212/212 [==============================] - 0s 690us/step - loss: 1.8575 - accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "212/212 [==============================] - 0s 702us/step - loss: 1.8613 - accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "212/212 [==============================] - 0s 744us/step - loss: 1.8382 - accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "212/212 [==============================] - 0s 744us/step - loss: 1.8494 - accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "212/212 [==============================] - 0s 708us/step - loss: 1.8472 - accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "212/212 [==============================] - 0s 679us/step - loss: 1.8467 - accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "212/212 [==============================] - 0s 685us/step - loss: 1.8461 - accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "212/212 [==============================] - 0s 704us/step - loss: 1.8543 - accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "212/212 [==============================] - 0s 705us/step - loss: 1.8502 - accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "212/212 [==============================] - 0s 691us/step - loss: 1.8434 - accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "212/212 [==============================] - 0s 716us/step - loss: 1.8537 - accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "212/212 [==============================] - 0s 701us/step - loss: 1.8463 - accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "212/212 [==============================] - 0s 659us/step - loss: 1.8494 - accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "212/212 [==============================] - 0s 734us/step - loss: 1.8456 - accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "212/212 [==============================] - 0s 722us/step - loss: 1.8475 - accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "212/212 [==============================] - 0s 688us/step - loss: 1.8437 - accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "212/212 [==============================] - 0s 719us/step - loss: 1.8423 - accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "212/212 [==============================] - 0s 749us/step - loss: 1.8415 - accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "212/212 [==============================] - 0s 832us/step - loss: 1.8443 - accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "212/212 [==============================] - 0s 775us/step - loss: 1.8404 - accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "212/212 [==============================] - 0s 813us/step - loss: 1.8411 - accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "212/212 [==============================] - 0s 782us/step - loss: 1.8427 - accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "212/212 [==============================] - 0s 779us/step - loss: 1.8451 - accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "212/212 [==============================] - 0s 706us/step - loss: 1.8429 - accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "212/212 [==============================] - 0s 706us/step - loss: 1.8404 - accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "212/212 [==============================] - 0s 838us/step - loss: 1.8468 - accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "212/212 [==============================] - 0s 733us/step - loss: 1.8433 - accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "212/212 [==============================] - 0s 793us/step - loss: 1.8466 - accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "212/212 [==============================] - 0s 825us/step - loss: 1.8475 - accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "212/212 [==============================] - 0s 760us/step - loss: 1.8412 - accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "212/212 [==============================] - 0s 786us/step - loss: 1.8518 - accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "212/212 [==============================] - 0s 813us/step - loss: 1.8492 - accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "212/212 [==============================] - 0s 675us/step - loss: 1.8464 - accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "212/212 [==============================] - 0s 789us/step - loss: 1.8433 - accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "212/212 [==============================] - 0s 750us/step - loss: 1.8464 - accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "212/212 [==============================] - 0s 750us/step - loss: 1.8459 - accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "212/212 [==============================] - 0s 802us/step - loss: 1.8382 - accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "212/212 [==============================] - 0s 774us/step - loss: 1.8452 - accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "212/212 [==============================] - 0s 734us/step - loss: 1.8466 - accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "212/212 [==============================] - 0s 806us/step - loss: 1.8405 - accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "212/212 [==============================] - 0s 823us/step - loss: 1.8445 - accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "212/212 [==============================] - 0s 691us/step - loss: 1.8439 - accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "212/212 [==============================] - 0s 689us/step - loss: 1.8468 - accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "212/212 [==============================] - 0s 748us/step - loss: 1.8418 - accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "212/212 [==============================] - 0s 765us/step - loss: 1.8460 - accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "212/212 [==============================] - 0s 737us/step - loss: 1.8525 - accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "212/212 [==============================] - 0s 733us/step - loss: 1.8394 - accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "212/212 [==============================] - 0s 758us/step - loss: 1.8440 - accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "212/212 [==============================] - 0s 741us/step - loss: 1.8446 - accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "212/212 [==============================] - 0s 755us/step - loss: 1.8445 - accuracy: 1.0000\n",
      "66/66 [==============================] - 0s 800us/step - loss: 1.8230 - accuracy: 1.0000\n",
      "Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# split into input (X) and output (y) variables\n",
    "X, y = df[['Thickness','Lsub', 'asub','bsub', 'Lcer', 'acer', 'bcer']].to_numpy(), df[['L', 'a', 'b']].to_numpy()\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=7, activation='relu'))\n",
    "model.add(Dense(3, activation='linear'))\n",
    "# compile the keras model\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=150, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "246876ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasRegressor' object has no attribute '__call__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# create model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mKerasRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# define the grid search parameters\u001b[39;00m\n\u001b[1;32m     12\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m100\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/wrappers/scikit_learn.py:362\u001b[0m, in \u001b[0;36mKerasRegressor.__init__\u001b[0;34m(self, build_fn, **sk_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_doc_inheritable\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, build_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msk_params):\n\u001b[1;32m    354\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKerasRegressor is deprecated, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse Sci-Keras (https://github.com/adriangb/scikeras) instead. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    361\u001b[0m     )\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuild_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msk_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/wrappers/scikit_learn.py:78\u001b[0m, in \u001b[0;36mBaseWrapper.__init__\u001b[0;34m(self, build_fn, **sk_params)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_fn \u001b[38;5;241m=\u001b[39m build_fn\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msk_params \u001b[38;5;241m=\u001b[39m sk_params\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43msk_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/wrappers/scikit_learn.py:95\u001b[0m, in \u001b[0;36mBaseWrapper.check_params\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     89\u001b[0m legal_params_fns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     90\u001b[0m     Sequential\u001b[38;5;241m.\u001b[39mfit,\n\u001b[1;32m     91\u001b[0m     Sequential\u001b[38;5;241m.\u001b[39mpredict,\n\u001b[1;32m     92\u001b[0m     Sequential\u001b[38;5;241m.\u001b[39mevaluate,\n\u001b[1;32m     93\u001b[0m ]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     legal_params_fns\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_fn, types\u001b[38;5;241m.\u001b[39mFunctionType\n\u001b[1;32m     98\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_fn, types\u001b[38;5;241m.\u001b[39mMethodType):\n\u001b[1;32m     99\u001b[0m     legal_params_fns\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KerasRegressor' object has no attribute '__call__'"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7, input_dim=7, activation='relu'))\n",
    "    model.add(Dense(3, activation='linear'))\n",
    "    # compile the keras model\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(model=create_model)\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca97cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
