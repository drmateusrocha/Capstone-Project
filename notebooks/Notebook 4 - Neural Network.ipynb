{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da70ee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2112 entries, 0 to 2111\n",
      "Data columns (total 13 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   ID         2112 non-null   int64  \n",
      " 1   Substrate  2112 non-null   object \n",
      " 2   Ceramic    2112 non-null   object \n",
      " 3   Thickness  2112 non-null   float64\n",
      " 4   Lsub       2112 non-null   float64\n",
      " 5   asub       2112 non-null   float64\n",
      " 6   bsub       2112 non-null   float64\n",
      " 7   Lcer       2112 non-null   float64\n",
      " 8   acer       2112 non-null   float64\n",
      " 9   bcer       2112 non-null   float64\n",
      " 10  L          2112 non-null   float64\n",
      " 11  a          2112 non-null   float64\n",
      " 12  b          2112 non-null   float64\n",
      "dtypes: float64(10), int64(1), object(2)\n",
      "memory usage: 214.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import math\n",
    "df = pd.read_csv('../data/data2.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d5ac35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Predict L\n",
    "X, y = df[['Thickness','Lsub', 'asub','bsub', 'Lcer', 'acer', 'bcer']], df[['L', 'a', 'b']]\n",
    "\n",
    "#Perform Scaling if needed\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc = StandardScaler()\n",
    "#X_sc = sc.fit_transform(X)\n",
    "\n",
    "#Splitting the data into train and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "099b5777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1689, 7) (1689, 3)\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "X, y = pd.DataFrame(df[['Thickness', 'Lsub', 'asub','bsub', 'Lcer', 'acer', 'bcer']]).to_numpy(), pd.DataFrame(df[['L','a','b']]).to_numpy()\n",
    "# summarize shape\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbec8088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1.401\n",
      ">1.702\n",
      ">1.644\n",
      ">1.617\n",
      ">1.392\n",
      ">2.056\n",
      ">1.326\n",
      ">1.516\n",
      ">1.344\n",
      ">1.907\n",
      ">1.314\n",
      ">1.764\n",
      ">1.551\n",
      ">1.366\n",
      ">1.726\n",
      "MAE: 1.575 (0.219)\n"
     ]
    }
   ],
   "source": [
    "# mlp for multi-output regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    \n",
    "    X, y = pd.DataFrame(df[['Thickness', 'Lsub', 'asub','bsub', 'Lcer', 'acer', 'bcer']]).to_numpy(), pd.DataFrame(df[['L','a','b']]).to_numpy()\n",
    "    return X, y\n",
    "\n",
    "# get the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs))\n",
    "\tmodel.compile(loss='mae', optimizer='adam')\n",
    "\treturn model\n",
    "\n",
    "# evaluate a model using repeated k-fold cross-validation\n",
    "def evaluate_model(X, y):\n",
    "\tresults = list()\n",
    "\tn_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\t# enumerate folds\n",
    "\tfor train_ix, test_ix in cv.split(X):\n",
    "\t\t# prepare data\n",
    "\t\tX_train, X_test = X[train_ix], X[test_ix]\n",
    "\t\ty_train, y_test = y[train_ix], y[test_ix]\n",
    "\t\t# define model\n",
    "\t\tmodel = get_model(n_inputs, n_outputs)\n",
    "\t\t# fit model\n",
    "\t\tmodel.fit(X_train, y_train, verbose=0, epochs=100)\n",
    "\t\t# evaluate model on test set\n",
    "\t\tmae = model.evaluate(X_test, y_test, verbose=0)\n",
    "\t\t# store result\n",
    "\t\tprint('>%.3f' % mae)\n",
    "\t\tresults.append(mae)\n",
    "\treturn results\n",
    "\n",
    "# load dataset\n",
    "X, y = get_dataset()\n",
    "# evaluate model\n",
    "results = evaluate_model(X, y)\n",
    "# summarize performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(results), std(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "617efeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes):\n",
    "    layers = []\n",
    "    \n",
    "    nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
    "    nodes = first_layer_nodes\n",
    "    for i in range(1, n_layers+1):\n",
    "        layers.append(math.ceil(nodes))\n",
    "        nodes = nodes + nodes_increment\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b42eef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createmodel(n_layers, first_layer_nodes, last_layer_nodes, activation_func, loss_func):\n",
    "    model = Sequential()\n",
    "    n_nodes = FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes)\n",
    "    for i in range(1, n_layers):\n",
    "        if i==1:\n",
    "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=activation_func))\n",
    "        else:\n",
    "            model.add(Dense(n_nodes[i-1], activation=activation_func))\n",
    "            \n",
    "    #Finally, the output layer should have a single node in binary classification\n",
    "    model.add(Dense(3, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss=loss_func, metrics = [\"accuracy\"]) #note: metrics could also be 'mse'\n",
    "    \n",
    "    return model\n",
    "\n",
    "##Wrap model into scikit-learn\n",
    "model =  KerasRegressor(build_fn=createmodel, verbose = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "943873ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_funcs = ['relu']#['sigmoid', 'relu', 'tanh'] \n",
    "loss_funcs = ['mean_squared_error'] #['mean_squared_error','hinge','mae']\n",
    "param_grid = dict(n_layers=[1,2], first_layer_nodes = [7, 5], last_layer_nodes = [5, 3],  activation_func = activation_funcs, loss_func = loss_funcs, batch_size = [50], epochs = [10])\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid, cv=10, n_jobs=-1, verbose=1, scoring = \"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42510034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exception calling callback for <Future at 0x14dd56213d0 state=finished raised BrokenProcessPool>\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 407, in _process_worker\n",
      "    call_item = call_queue.get(block=True, timeout=timeout)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\multiprocessing\\queues.py\", line 117, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\multiprocessing\\connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\multiprocessing\\connection.py\", line 323, in _recv_bytes\n",
      "    return self._get_more_data(ov, maxsize)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\multiprocessing\\connection.py\", line 349, in _get_more_data\n",
      "    f.write(ov.getbuffer())\n",
      "MemoryError\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 625, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py\", line 359, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py\", line 794, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 531, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\reusable_executor.py\", line 177, in submit\n",
      "    return super(_ReusablePoolExecutor, self).submit(\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 1115, in submit\n",
      "    raise self._flags.broken\n",
      "joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
      "Exception in thread ExecutorManagerThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 566, in run\n",
      "    self.terminate_broken(bpe)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 750, in terminate_broken\n",
      "    self.kill_workers(reason=\"broken executor\")\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 781, in kill_workers\n",
      "    recursive_terminate(p)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\backend\\utils.py\", line 26, in recursive_terminate\n",
      "    _recursive_terminate_with_psutil(process)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\joblib\\externals\\loky\\backend\\utils.py\", line 33, in _recursive_terminate_with_psutil\n",
      "    children = psutil.Process(process.pid).children(recursive=True)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\psutil\\__init__.py\", line 278, in wrapper\n",
      "    return fun(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\groch\\.conda\\envs\\GPU\\lib\\site-packages\\psutil\\__init__.py\", line 906, in children\n",
      "    ppid_map = _ppid_map()\n",
      "OSError: [WinError 1455] The paging file is too small for this operation to complete\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d627c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ae18a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(grid.cv_results_)\n",
    "score_df.sort_values(by='rank_test_score').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6c9e299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-24.490260</td>\n",
       "      <td>14.922815</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-25.036329</td>\n",
       "      <td>11.882321</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-23.875925</td>\n",
       "      <td>11.804245</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-24.278635</td>\n",
       "      <td>9.413140</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-51.353558</td>\n",
       "      <td>10.550953</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-74.233941</td>\n",
       "      <td>74.533497</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-161.150762</td>\n",
       "      <td>129.050585</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-60.770346</td>\n",
       "      <td>55.504804</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-206.444029</td>\n",
       "      <td>157.495241</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-165.809924</td>\n",
       "      <td>179.319228</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-117.522153</td>\n",
       "      <td>94.180486</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-725.065265</td>\n",
       "      <td>826.535223</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-241.122491</td>\n",
       "      <td>193.138596</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-79.065467</td>\n",
       "      <td>31.857363</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-94.885411</td>\n",
       "      <td>73.731444</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-110.547735</td>\n",
       "      <td>103.795524</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-1660.015878</td>\n",
       "      <td>927.089387</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-727.792342</td>\n",
       "      <td>694.000003</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-1102.576804</td>\n",
       "      <td>872.194213</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-1484.946265</td>\n",
       "      <td>176.345959</td>\n",
       "      <td>{'activation_func': 'relu', 'batch_size': 50, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score  std_test_score  \\\n",
       "0               NaN             NaN   \n",
       "1        -24.490260       14.922815   \n",
       "2               NaN             NaN   \n",
       "3        -25.036329       11.882321   \n",
       "4               NaN             NaN   \n",
       "5        -23.875925       11.804245   \n",
       "6               NaN             NaN   \n",
       "7        -24.278635        9.413140   \n",
       "8               NaN             NaN   \n",
       "9        -51.353558       10.550953   \n",
       "10              NaN             NaN   \n",
       "11       -74.233941       74.533497   \n",
       "12              NaN             NaN   \n",
       "13      -161.150762      129.050585   \n",
       "14              NaN             NaN   \n",
       "15       -60.770346       55.504804   \n",
       "16              NaN             NaN   \n",
       "17      -206.444029      157.495241   \n",
       "18              NaN             NaN   \n",
       "19      -165.809924      179.319228   \n",
       "20              NaN             NaN   \n",
       "21      -117.522153       94.180486   \n",
       "22              NaN             NaN   \n",
       "23      -725.065265      826.535223   \n",
       "24              NaN             NaN   \n",
       "25      -241.122491      193.138596   \n",
       "26              NaN             NaN   \n",
       "27       -79.065467       31.857363   \n",
       "28              NaN             NaN   \n",
       "29       -94.885411       73.731444   \n",
       "30              NaN             NaN   \n",
       "31      -110.547735      103.795524   \n",
       "32              NaN             NaN   \n",
       "33     -1660.015878      927.089387   \n",
       "34              NaN             NaN   \n",
       "35      -727.792342      694.000003   \n",
       "36              NaN             NaN   \n",
       "37     -1102.576804      872.194213   \n",
       "38              NaN             NaN   \n",
       "39     -1484.946265      176.345959   \n",
       "\n",
       "                                               params  \n",
       "0   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "1   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "2   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "3   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "4   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "5   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "6   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "7   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "8   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "9   {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "10  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "11  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "12  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "13  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "14  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "15  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "16  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "17  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "18  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "19  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "20  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "21  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "22  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "23  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "24  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "25  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "26  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "27  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "28  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "29  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "30  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "31  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "32  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "33  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "34  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "35  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "36  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "37  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "38  {'activation_func': 'relu', 'batch_size': 50, ...  \n",
       "39  {'activation_func': 'relu', 'batch_size': 50, ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate Deep ANN model \n",
    "def make_regression_ann(Optimizer_trial):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=15, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(3, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Optimizer_trial)\n",
    "    return model\n",
    " \n",
    "###########################################\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    " \n",
    "# Listing all the parameters to try\n",
    "Parameter_Trials={'batch_size':[10,20,30],\n",
    "                      'epochs':[10,20],\n",
    "                    'Optimizer_trial':['adam', 'rmsprop']\n",
    "                 }\n",
    " \n",
    "# Creating the regression ANN model\n",
    "RegModel=KerasRegressor(make_regression_ann, verbose=0)\n",
    " \n",
    "###########################################\n",
    "from sklearn.metrics import make_scorer\n",
    " \n",
    "# Defining a custom function to calculate accuracy\n",
    "def Accuracy_Score(orig,pred):\n",
    "    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))\n",
    "    print('#'*70,'Accuracy:', 100-MAPE)\n",
    "    return(100-MAPE)\n",
    " \n",
    "custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n",
    " \n",
    "#########################################\n",
    "# Creating the Grid search space\n",
    "# See different scoring methods by using sklearn.metrics.SCORERS.keys()\n",
    "grid_search=GridSearchCV(estimator=RegModel, \n",
    "                         param_grid=Parameter_Trials, \n",
    "                         scoring='r2', \n",
    "                         cv=5)\n",
    " \n",
    "#########################################\n",
    "# Measuring how much time it took to find the best params\n",
    "import time\n",
    "StartTime=time.time()\n",
    " \n",
    "# Running Grid Search for different paramenters\n",
    "grid_search.fit(X,y, verbose=1)\n",
    " \n",
    "EndTime=time.time()\n",
    "print(\"########## Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes')\n",
    " \n",
    "print('### Printing Best parameters ###')\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c19284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train ,batch_size = 10, epochs = 20, verbose=0)\n",
    " \n",
    "\n",
    "Test_Data=PredictorScalerFit.inverse_transform(X_test)\n",
    " \n",
    "TestingData=pd.DataFrame(data=Test_Data, columns=Predictors)\n",
    "TestingData['L']=y_test_orig\n",
    "TestingData['PredictedL']=Predictions\n",
    "TestingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ec514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the absolute percent error\n",
    "APE=100*(abs(TestingData['L']-TestingData['PredictedL'])/TestingData['L'])\n",
    "TestingData['APE']=APE\n",
    " \n",
    "print('The Accuracy of ANN model is:', 100-np.mean(APE))\n",
    "TestingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(TestingData['L'], TestingData['PredictedL'], edgecolors=(0, 0, 0))\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], \"k--\", lw=4)\n",
    "ax.set_xlabel(\"Measured\")\n",
    "ax.set_ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5dea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(TestingData['L'], TestingData['APE'], edgecolors=(0, 0, 0))\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], \"k--\", lw=4)\n",
    "ax.set_xlabel(\"Measured\")\n",
    "ax.set_ylabel(\"APE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ebe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and training sets.\n",
    "\n",
    "x_label = ['Lsub','asub', 'bsub', 'Lcer','acer','bcer', 'Thickness']\n",
    "y_label = ['L']\n",
    "np.random.seed(100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[x_label],df[y_label],test_size=0.2)\n",
    "# Print the dimensions\n",
    "print('Training set dimensions X, y: ' + str(X_train.shape) + ' ' +str(y_train.shape))\n",
    "print('Test set dimensions X, y: ' + str(X_test.shape) + ' '+ str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regression model in Keras\n",
    "def regression_model():\n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=15, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(3, kernel_initializer='normal'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Use KerasRegressor wrapper (from Keras to sklearn)\n",
    "# The packages we use are meant to be run with sklearn models\n",
    "estimator = KerasRegressor(build_fn=regression_model, validation_split = 0.2, batch_size=100, epochs=100, verbose=0)\n",
    "history = estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e51439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history loss\n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(estimator, random_state=1).fit(X_train,y_train)\n",
    "eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp for multi-output regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    x_label = ['Lsub','asub', 'bsub', 'Lcer','acer','bcer', 'Thickness']\n",
    "    y_label = ['L','a','b']\n",
    "    X, y = df[x_label], df[y_label]\n",
    "    return X, y\n",
    "# summarize shape\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and training sets.\n",
    "\n",
    "x_label = ['Lsub','asub', 'bsub', 'Lcer','acer','bcer', 'Thickness']\n",
    "y_label = ['L','a','b']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[x_label],df[y_label],test_size=0.2)\n",
    "# Print the dimensions\n",
    "print('Training set dimensions X, y: ' + str(X_train.shape) + ' ' +str(y_train.shape))\n",
    "print('Test set dimensions X, y: ' + str(X_test.shape) + ' '+ str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regression model in Keras\n",
    "def regression_model():\n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=5, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    adam = optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Use KerasRegressor wrapper (from Keras to sklearn)\n",
    "# The packages we use are meant to be run with sklearn models\n",
    "estimator = KerasRegressor(build_fn=regression_model, validation_split = 0.2, batch_size=100, epochs=100, verbose=0)\n",
    "history = estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f736ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(estimator, random_state=1).fit(X_train,y_train)\n",
    "eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b68e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ca2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# SHAP expects model functions to take a 2D numpy array as input, so we define a wrapper function around the original Keras predict function.\n",
    "def f_wrapper(X):\n",
    "    return estimator.predict(X).flatten()\n",
    "\n",
    "# Too many input data - use a random slice\n",
    "# rather than use the whole training set to estimate expected values, we summarize with\n",
    "# a set of weighted kmeans, each weighted by the number of points they represent.\n",
    "X_train_summary = shap.kmeans(X_train, 20)\n",
    "\n",
    "# Compute Shap values\n",
    "explainer = shap.KernelExplainer(f_wrapper,X_train_summary)\n",
    "\n",
    "# Make plot with combined shap values\n",
    "# The training set is too big so let's sample it. We get enough point to draw conclusions\n",
    "X_train_sample = X_train.sample(400)\n",
    "shap_values  = explainer.shap_values(X_train_sample)\n",
    "shap.summary_plot(shap_values, X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc804508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X, y = get_dataset()\n",
    "n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "# get model\n",
    "model = get_model(n_inputs, n_outputs)\n",
    "# fit the model on all data\n",
    "model.fit(X, y, verbose=0, epochs=100)\n",
    "# make a prediction for new data\n",
    "row = []\n",
    "newX = asarray([row])\n",
    "yhat = model.predict(newX)\n",
    "print('Predicted: %s' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf345cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "X, y = df[['Thickness','Lsub', 'asub','bsub', 'Lcer', 'acer', 'bcer']].to_numpy(), df[['L', 'a', 'b']].to_numpy()\n",
    "# summarize shape\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34acd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(units=15, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(3, kernel_initializer='normal'))\n",
    "model.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87694e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=15, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(3, kernel_initializer='normal'))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dead839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model using repeated k-fold cross-validation\n",
    "def evaluate_model(X, y):\n",
    "    results = []\n",
    "    n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        # prepare data\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        # define model\n",
    "        model = get_model(n_inputs, n_outputs)\n",
    "        # fit model\n",
    "        model.fit(X_train, y_train, verbose=10, epochs=15)\n",
    "        # evaluate model on test set\n",
    "        mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "        # store result\n",
    "        print('>%.3f' % mae)\n",
    "        results.append(mae)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe155bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "results = evaluate_model(X, y)\n",
    "# summarize performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(results), std(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction for new data\n",
    "row = [0.3,70,2.2,9,71, 0.2, 10]\n",
    "newX = np.array([row])\n",
    "yhat = model.predict(newX)\n",
    "print('Predicted: %s' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce51e6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "Lf = 10\n",
    "Lcer = 7\n",
    "Lsub = 20\n",
    "np.clip(Lf, np.min([Lcer,Lsub]), np.max([Lcer,Lsub]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83ef9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([10,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c137c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
